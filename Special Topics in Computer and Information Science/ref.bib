@article{ZENG2016416,
title = {Color image classification via quaternion principal component analysis network},
journal = {Neurocomputing},
volume = {216},
pages = {416-428},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S092523121630827X},
author = {Rui Zeng and Jiasong Wu and Zhuhong Shao and Yang Chen and Beijing Chen and Lotfi Senhadji and Huazhong Shu},
keywords = {Deep learning, Convolutional neural network, Quaternion, QPCANet, PCANet, Color image classification},
abstract = {The principal component analysis network (PCANet), which is one of the recently proposed deep learning architectures, achieves the state-of-the-art classification accuracy in various datasets and reveals a simple baseline for deep learning networks. However, the performance of PCANet may be degraded when dealing with color images due to the fact that the architecture of PCANet cannot properly utilize the spatial relationship between each color channel in three dimensional color image. In this paper, a quaternion principal component analysis network (QPCANet), which extends PCANet by using quaternion theory, is proposed for color image classification. Compared to PCANet, the proposed QPCANet takes into account the spatial distribution information of RGB channels in color images and ensures larger amount of intra-class invariance by using quaternion domain representation for color images. Experiments conducted on different color image datasets such as UC Merced Land Use, Georgia Tech face, CURet and Caltech-101 have revealed that the proposed QPCANet generally achieves higher classification accuracy than PCANet in color image classification task. The experimental results also verify that QPCANet has much better rotation invariance than PCANet when color image dataset contains lots of rotation information and demonstrate even a simple one-layer QPCANet may obtain satisfactory accuracy when compared with two-layer PCANet.}
}
@INPROCEEDINGS{6607436,  author={Yu, Licheng and Xu, Yi and Xu, Hongteng and Zhang, Hao},  booktitle={2013 IEEE International Conference on Multimedia and Expo (ICME)},   title={Quaternion-based sparse representation of color image},   year={2013},  volume={},  number={},  pages={1-7},  abstract={In this paper, we propose a quaternion-based sparse representation model for color images and its corresponding dictionary learning algorithm. Differing from traditional sparse image models, which represent RGB channels separately or process RGB channels as a concatenated real vector, the proposed model describes the color image as a quaternion vector matrix, where each color pixel is encoded as a quaternion unit and thus the inter-relationship among RGB channels is well preserved. Correspondingly, we propose a quaternion-based dictionary learning algorithm using a socalled K-QSVD method. It conducts the sparse basis selection in quaternion vector space, providing a kind of vectorial representation for the inherent color structures rather than a scalar representation via current sparse image models. The proposed sparse model is validated in the applications of color image denoising and inpainting. The experimental results demonstrate that our sparse image model avoids the hue bias phenomenon successfully and shows its potential as a powerful tool in color image analysis and processing domain.},  keywords={},  doi={10.1109/ICME.2013.6607436},  ISSN={1945-788X},  month={July},}

@inproceedings{Cootes2000AnIT,
  title={An Introduction to Active Shape Models},
  author={Tim F. Cootes},
  year={2000}
}
@ARTICLE{1717463,
  author={Ahonen, T. and Hadid, A. and Pietikainen, M.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Face Description with Local Binary Patterns: Application to Face Recognition}, 
  year={2006},
  volume={28},
  number={12},
  pages={2037-2041},
  abstract={This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features. The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. The performance of the proposed method is assessed in the face recognition problem under different challenges. Other applications and several extensions are also discussed},
  keywords={},
  doi={10.1109/TPAMI.2006.244},
  ISSN={1939-3539},
  month={Dec},}

@INPROCEEDINGS{5539992,
  author={Cao, Zhimin and Yin, Qi and Tang, Xiaoou and Sun, Jian},
  booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition}, 
  title={Face recognition with learning-based descriptor}, 
  year={2010},
  volume={},
  number={},
  pages={2707-2714},
  abstract={We present a novel approach to address the representation issue and the matching issue in face recognition (verification). Firstly, our approach encodes the micro-structures of the face by a new learning-based encoding method. Unlike many previous manually designed encoding methods (e.g., LBP or SIFT), we use unsupervised learning techniques to learn an encoder from the training examples, which can automatically achieve very good tradeoff between discriminative power and invariance. Then we apply PCA to get a compact face descriptor. We find that a simple normalization mechanism after PCA can further improve the discriminative ability of the descriptor. The resulting face representation, learning-based (LE) descriptor, is compact, highly discriminative, and easy-to-extract. To handle the large pose variation in real-life scenarios, we propose a pose-adaptive matching method that uses pose-specific classifiers to deal with different pose combinations (e.g., frontal v.s. frontal, frontal v.s. left) of the matching face pair. Our approach is comparable with the state-of-the-art methods on the Labeled Face in Wild (LFW) benchmark (we achieved 84.45% recognition rate), while maintaining excellent compactness, simplicity, and generalization aability across different datasets.},
  keywords={},
  doi={10.1109/CVPR.2010.5539992},
  ISSN={1063-6919},
  month={June},}

  @InProceedings{10.1007/BFb0054760,
author="Cootes, T. F.
and Edwards, G. J.
and Taylor, C. J.",
editor="Burkhardt, Hans
and Neumann, Bernd",
title="Active appearance models",
booktitle="Computer Vision --- ECCV'98",
year="1998",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="484--498",
abstract="We demonstrate a novel method of interpreting images using an Active Appearance Model (AAM). An AAM contains a statistical model of the shape and grey-level appearance of the object of interest which can generalise to almost any valid example. During a training phase we learn the relationship between model parameter displacements and the residual errors induced between a training image and a synthesised model example. To match to an image we measure the current residuals and use the model to predict changes to the current parameters, leading to a better fit. A good overall match is obtained in a few iterations, even from poor starting estimates. We describe the technique in detail and give results of quantitative performance tests. We anticipate that the AAM algorithm will be an important method for locating deformable objects in many applications.",
isbn="978-3-540-69235-5"
}
@INPROCEEDINGS{6619290,
  author={Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
  booktitle={2013 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Deep Convolutional Network Cascade for Facial Point Detection}, 
  year={2013},
  volume={},
  number={},
  pages={3476-3483},
  doi={10.1109/CVPR.2013.446}}

  @ARTICLE{726791,  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},  journal={Proceedings of the IEEE},   title={Gradient-based learning applied to document recognition},   year={1998},  volume={86},  number={11},  pages={2278-2324},  abstract={Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},  keywords={},  doi={10.1109/5.726791},  ISSN={1558-2256},  month={Nov},}
@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@article{brusilovsky:simonyan2014very,
  abstract = {The used CNN implementation (VGGNet) by the main paper (used)},
  added-at = {2020-06-08T19:18:30.000+0200},
  author = {Simonyan, Karen and Zisserman, Andrew},
  biburl = {https://www.bibsonomy.org/bibtex/2bc3ee27a1dd159f48b10ac3555879865/buch_jon},
  citeulike-article-id = {14183517},
  interhash = {4e6fa56cb7cf99400d5701543ee228de},
  intrahash = {bc3ee27a1dd159f48b10ac3555879865},
  journal = {arXiv preprint arXiv:1409.1556},
  keywords = {},
  posted-at = {2016-11-14 05:20:24},
  priority = {2},
  timestamp = {2020-06-08T19:18:30.000+0200},
  title = {{Very deep convolutional networks for large-scale image recognition}},
  year = 2014
}
@INPROCEEDINGS{7780459,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  abstract={Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8Ã— deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords={},
  doi={10.1109/CVPR.2016.90},
  ISSN={1063-6919},
  month={June},}